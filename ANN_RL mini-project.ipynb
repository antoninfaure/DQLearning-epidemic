{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "wdEXoCB8FXCtj8gN0SYOwR",
     "report_properties": {
      "rowId": "U5fGYqNuBmGmfvzX3AUOZC"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Mini-project 1: Deep Q-learning for Epidemic Mitigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "yxT5wHtvLeP0DTwpKQmCuI",
     "report_properties": {
      "rowId": "ZISnysiDStsazjDQdpVXkE"
     },
     "type": "MD"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "kw5ZvUEfOAjmMgOeifSlMO",
     "report_properties": {
      "rowId": "6rRndAIeGJ2DzMkTUyPBhf"
     },
     "type": "CODE"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.agent import Agent\n",
    "from epidemic_env.dynamics import ModelDynamics\n",
    "from epidemic_env.env import Env\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#@formatter:off\n",
    "%matplotlib inline\n",
    "#@formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# detect the device (CPU or GPU)\n",
    "device = 'cpu'  # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_results(total, actions, cities):\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    ax_leftstate = plt.subplot2grid(shape=(9, 2), loc=(0, 0), rowspan=4)\n",
    "    ax_leftobs = plt.subplot2grid(shape=(9, 2), loc=(4, 0), rowspan=3)\n",
    "    ax_leftactions = plt.subplot2grid(shape=(9, 2), loc=(7, 0), rowspan=2)\n",
    "    ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "    ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1, 9)]\n",
    "    ax_right = {k: ax_right[_id] for _id, k in enumerate(cities.keys())}\n",
    "\n",
    "    [ax_leftstate.plot(y) for y in total.values()]\n",
    "    ax_leftstate.legend(total.keys())\n",
    "    ax_leftstate.set_title('Full state')\n",
    "    ax_leftstate.set_ylabel('number of people in each state')\n",
    "\n",
    "    [ax_leftobs.plot(total[y]) for y in ['infected', 'dead']]\n",
    "    ax_leftobs.legend(['infected', 'dead'])\n",
    "    ax_leftobs.set_title('Observable state')\n",
    "    ax_leftobs.set_ylabel('number of people in each state')\n",
    "\n",
    "    ax_leftactions.imshow(np.array([v for v in actions.values()]).astype(np.uint8), aspect='auto')\n",
    "    ax_leftactions.set_title('Actions')\n",
    "    ax_leftactions.set_yticks([0, 1, 2, 3])\n",
    "    ax_leftactions.set_yticklabels(list(actions.keys()))\n",
    "    ax_leftactions.set_xlabel('time (in weeks)')\n",
    "\n",
    "    [ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "    [ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "    [ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "    [ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "    # ax_right['Zürich'].set_xlabel('time (in weeks)')\n",
    "    # ax_right['Zürich'].xaxis.set_major_locator(MultipleLocator(2.000))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_infos(infos_list):\n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    total = {p: np.array([getattr(l.total, p) for l in infos_list]) for p in dyn.parameters}\n",
    "    cities = {c: {p: np.array([getattr(l.city[c], p) for l in infos_list]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a: np.array([l.action[a] for l in infos_list]) for a in infos_list[0].action.keys()}\n",
    "\n",
    "    return total, actions, cities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')\n",
    "env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "          action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "          observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "          )\n",
    "obs, info = env.reset(seed=0)\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class BasicPolicy(Agent):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        return {\n",
    "            'confinement': False,\n",
    "            'isolation': False,\n",
    "            'hospital': False,\n",
    "            'vaccinate': False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Run basic simulation (no actions) \"\"\"\n",
    "infos_list = []\n",
    "policy = BasicPolicy()\n",
    "for i in range(30):\n",
    "    action = policy.act(obs, reward)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    infos_list.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "total, actions, cities = parse_infos(infos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_results(total, actions, cities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 1.a).1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(total['suceptible'], label='suceptible')\n",
    "plt.plot(total['exposed'], label='exposed')\n",
    "plt.plot(total['infected'], label='infected')\n",
    "plt.plot(total['recovered'], label='recovered')\n",
    "plt.plot(total['dead'], label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 1.a).2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(total['infected'], label='infected')\n",
    "plt.plot(total['dead'], label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 1.a).3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=9, ncols=1, figsize=(10, 10))\n",
    "for i, c in enumerate(cities.keys()):\n",
    "    ax[i].plot(cities[c]['infected'], label='infected')\n",
    "    ax[i].plot(cities[c]['dead'], label='dead')\n",
    "    ax[i].set_ylabel(c)\n",
    "    if i < 8:\n",
    "        ax[i].xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "ax[0].legend()\n",
    "plt.xlabel(\"Time [week]\")\n",
    "fig.supylabel(\"# of people\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Discuss the evolution of the variables over time.\n",
    "TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 2 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class RussoPolicy(Agent):\n",
    "    def __init__(self):\n",
    "        self.total_infected = 0\n",
    "        self.confinment_remaining = 0\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_infected = 0\n",
    "        self.confinment_remaining = 0\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        self.total_infected = sum(obs.total.infected)\n",
    "        confinment = False\n",
    "\n",
    "        if self.total_infected > 20000 and self.confinment_remaining == 0:\n",
    "            confinment = True\n",
    "            self.confinment_remaining = 4\n",
    "        elif self.confinment_remaining > 0:\n",
    "            confinment = True\n",
    "            self.confinment_remaining -= 1\n",
    "\n",
    "        return {\n",
    "            'confinement': confinment,\n",
    "            'isolation': False,\n",
    "            'hospital': False,\n",
    "            'vaccinate': False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')\n",
    "env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "          action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "          observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "          )\n",
    "obs, info = env.reset(seed=0)\n",
    "action = None\n",
    "infos_list = []\n",
    "policy = RussoPolicy()\n",
    "actions = []\n",
    "reward = [[0]]\n",
    "for i in range(30):\n",
    "    action = policy.act(obs, reward[0][0])\n",
    "    actions.append(action['confinement'])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    infos_list.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "total_russo, actions_russo, cities_russo = parse_infos(infos_list)\n",
    "plot_results(total_russo, actions_russo, cities_russo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "totals = [info.total for info in infos_list]\n",
    "suceptibles = [total.suceptible for total in totals]\n",
    "exposeds = [total.exposed for total in totals]\n",
    "infected = [total.infected for total in totals]\n",
    "recovereds = [total.recovered for total in totals]\n",
    "deads = [total.dead for total in totals]\n",
    "\n",
    "plt.plot(suceptibles, label='suceptible')\n",
    "plt.plot(exposeds, label='exposed')\n",
    "plt.plot(infected, label='infected')\n",
    "plt.plot(recovereds, label='recovered')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(infected, label='infected')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=9, ncols=1, figsize=(10, 10))\n",
    "for i, c in enumerate(cities_russo.keys()):\n",
    "    ax[i].plot(cities_russo[c]['infected'], label='infected')\n",
    "    ax[i].plot(cities_russo[c]['dead'], label='dead')\n",
    "    ax[i].set_ylabel(c)\n",
    "    if i < 8:\n",
    "        ax[i].xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "ax[0].legend()\n",
    "plt.xlabel(\"Time [week]\")\n",
    "fig.supylabel(\"# of people\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ax_actions = plt.subplot2grid(shape=(18, 1), loc=(0, 0), colspan=1, rowspan=9)\n",
    "ax_actions.imshow(np.array([v for v in actions_russo.values()]).astype(np.uint8), aspect='auto')\n",
    "ax_actions.set_title('Actions')\n",
    "ax_actions.set_yticks([0, 1, 2, 3])\n",
    "ax_actions.set_yticklabels(list(actions_russo.keys()))\n",
    "ax_actions.set_xlabel('Time [week]')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 2 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def run_episode(policy: Agent, env: Env, weeks: int = 30, seed: int = 0) -> (list, list, ...):\n",
    "    # We pass a seed to the env to ensure reproductibility\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    policy.reset()\n",
    "    cumulative_reward = 0\n",
    "    action_list, obs_list, reward_list = [], [], []\n",
    "    reward = torch.tensor([[0]])\n",
    "    for i in range(weeks):\n",
    "        action = policy.act(obs, reward)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        cumulative_reward = reward + cumulative_reward\n",
    "        action_list.append(action)\n",
    "        obs_list.append(obs)\n",
    "        reward_list.append(reward)\n",
    "    return action_list, obs_list, cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy: Agent, env: Env, iterations: int = 50) -> (list, list, list):\n",
    "    number_of_confined_days_list = []\n",
    "    cumulative_reward_list = []\n",
    "    number_of_total_deaths_list = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # action_list, observation_list\n",
    "        action_list, obs_list, cumulative_reward = run_episode(policy, env, weeks=30, seed=0)\n",
    "\n",
    "        # get number of days confined\n",
    "        number_of_confined_days = np.sum([7 if action['confinement'] else 0 for action in action_list])\n",
    "\n",
    "        # get last observation and sum dead\n",
    "        number_of_deaths = np.sum(obs_list[-1].total.dead)\n",
    "\n",
    "        # append to lists\n",
    "        cumulative_reward_list.append(cumulative_reward)\n",
    "        number_of_confined_days_list.append(number_of_confined_days)\n",
    "        number_of_total_deaths_list.append(number_of_deaths)\n",
    "\n",
    "    cumulative_reward_list = [tensor[0, 0] for tensor in cumulative_reward_list]\n",
    "    return number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list = evaluate_policy(RussoPolicy(), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram(number_of_confined_days_list: list, cumulative_reward_list: str, number_of_total_deaths_list: list):\n",
    "    number_of_confined_days_array = np.array(number_of_confined_days_list, dtype=float).reshape((-1, 1))\n",
    "    cumulative_reward_array = np.array(cumulative_reward_list, dtype=float).reshape((-1, 1))\n",
    "    number_of_total_deaths_array = np.array(number_of_total_deaths_list, dtype=float).reshape((-1, 1))\n",
    "\n",
    "    \"\"\" Plot results \"\"\"\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(18, 8))\n",
    "\n",
    "    def hist_avg(ax, data, title):\n",
    "        ymax = 50\n",
    "        if title == 'deaths':\n",
    "            x_range = (1000, 200000)\n",
    "            title = '# of total deaths'\n",
    "        elif title == 'cumulative rewards':\n",
    "            x_range = (-300, 300)\n",
    "            title = 'Cumulative rewards'\n",
    "        elif 'days' in title:\n",
    "            x_range = (0, 200)\n",
    "            title = '# of confined days'\n",
    "        else:\n",
    "            raise ValueError(f'{title} is not a valid title')\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylim(0, ymax)\n",
    "        ax.set_ylabel('Number of episodes')\n",
    "        ax.vlines([np.mean(data)], 0, ymax, color='red')\n",
    "        ax.hist(data, bins=60, range=x_range)\n",
    "\n",
    "    hist_avg(ax[0], number_of_total_deaths_array, 'deaths')\n",
    "    hist_avg(ax[1], cumulative_reward_array, 'cumulative rewards')\n",
    "    hist_avg(ax[2], number_of_confined_days_array, 'confined days')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "zb77IxDo0RtXa4SAKFXit6",
     "type": "MD"
    }
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# For each deep-learning policy that you train in this project we recommend that you use the following\n",
    "# hyperparameters. If you wish so, you are free to test other values, but as searching through the hyperparameterspace can be quite a tedious experience we are giving you values that we know will allow the algorithm to converge\n",
    "# to a good policy\n",
    "def create_model(input_dim: int, output_dim: int, activation: nn.Module = nn.ReLU()):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, 64),\n",
    "        activation,\n",
    "        nn.Linear(64, 32),\n",
    "        activation,\n",
    "        nn.Linear(32, 16),\n",
    "        activation,\n",
    "        nn.Linear(16, output_dim)\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "TLgLHRGmxjSNps34SAUioB",
     "type": "CODE"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Note unless special measures are taken, the training of neural networks is non-deterministic in most deeplearning libraries. To ensure that your results are reproducible you will thus need to seed not only the environment but also your deep-learning library. Refer to the jupyter tutorial notebook for instructions.\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Mzq25aVMNiMHb1LhlWBkSC",
     "type": "CODE"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from epidemic_env.dynamics import Observation\n",
    "\n",
    "\n",
    "def observation2tensor(obs: Observation):\n",
    "    \"\"\"\n",
    "    Convert an Observation object to a torch tensor so it can be used to feed a nn\n",
    "    \"\"\"\n",
    "    # 9 cities, 2 for dead/infected, 7=|city.dead| days per week\n",
    "    # -> 2*7*9 sized vector\n",
    "    total_population = sum(obs.pop.values())\n",
    "    obs = torch.tensor([x for city in obs.city.values() for x in city.dead + city.infected]).float()\n",
    "\n",
    "    # scale the observation and apply ^1/4 to avoid tiny values\n",
    "    obs = (obs / total_population) ** .25\n",
    "    return obs.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ntcVgxLlpexN2KKUbg7sr8",
     "type": "MD"
    }
   },
   "source": [
    "### Question 3.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "PYeqWR4kxG0tUecp2h9PvA",
     "type": "CODE"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "epsilon = .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "yBp7CwtCMcWzXgndGgDtoH",
     "type": "CODE"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class AgentDQN(Agent):\n",
    "    \"\"\"\n",
    "    Implemented by following this tutorial : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        options = kwargs.get('options', {})\n",
    "\n",
    "        # set the seed for reproducibility\n",
    "        torch.manual_seed(options.get('seed', 0))\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "\n",
    "        self.update_episode = options.get('update_episode', 5)\n",
    "        self.learning_rate = options.get('learning_rate', 5e-3)\n",
    "        self.discount_factor = options.get('discount_factor', .9)\n",
    "        self.buffer_size = options.get('buffer_size', 2e4)\n",
    "        self.batch_size = options.get('batch_size', 2048)\n",
    "        self.epsilon = options.get('epsilon', .7)\n",
    "\n",
    "        self.episode = 0\n",
    "        self.buffer = []  # contains (states, actions, next_state, reward) tuples\n",
    "        self.last_obs, self.last_action, self.last_reward = None, None, None\n",
    "\n",
    "        in_dim, out_dim = 2 * 9 * 7, 2\n",
    "        self.Q_network = create_model(in_dim, out_dim, nn.ReLU())\n",
    "        self.Q_target_network = create_model(in_dim, out_dim, nn.ReLU())\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_network.parameters(), lr=self.learning_rate, amsgrad=True)\n",
    "\n",
    "        self.best_eval_reward = -float('inf')  # for model saving\n",
    "\n",
    "        self.savepath = options.get('savepath', None)\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        self.Q_network.load_state_dict(torch.load(savepath))\n",
    "        # load the target network with the same weights\n",
    "        self.Q_target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0\n",
    "        transitions = torch.randperm(len(self.buffer))[:self.batch_size]\n",
    "        transitions = [self.buffer[i] for i in transitions]\n",
    "        state_batch, action_batch, next_state_batch, reward_batch = zip(*transitions)\n",
    "        state_batch, reward_batch = map(torch.stack, (state_batch, reward_batch))\n",
    "        action_batch = torch.tensor(action_batch).reshape(-1, 1)\n",
    "        state_batch = state_batch.to(device)\n",
    "        reward_batch = reward_batch.reshape((-1, 1)).to(device)  # reshape to (batch_size, 1)\n",
    "\n",
    "        next_state_batch = [obs for obs in next_state_batch if obs is not None]\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "        state_action_values = self.Q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            if len(next_state_batch) > 0:\n",
    "                non_final_mask = torch.tensor(tuple(map(lambda t: t[2] is not None, transitions)), device=device,\n",
    "                                              dtype=torch.bool)\n",
    "                next_state_batch = torch.stack(next_state_batch).to(device)\n",
    "                next_state_values[non_final_mask] = self.Q_target_network(next_state_batch).max(1)[0]\n",
    "\n",
    "        next_state_values = next_state_values.reshape((-1, 1))\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update the target network every update_episode steps\n",
    "        if self.episode % self.update_episode == 0:\n",
    "            self.Q_target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "        # do not do in place gradient clipping\n",
    "        return loss.item()\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        torch.save(self.Q_network.state_dict(), savepath)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Called at the beginning of each episode\n",
    "        \"\"\"\n",
    "        if self.last_reward is not None:\n",
    "            self.buffer.append((self.last_obs, self.last_action, None, self.last_reward))\n",
    "        self.last_reward = None\n",
    "        self.last_obs = None\n",
    "        self.last_action = None\n",
    "        self.episode += 1\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        # cast the observation\n",
    "        obs = observation2tensor(obs)\n",
    "        # get the next action\n",
    "        # epsilon-greedy policy\n",
    "        if torch.rand(1) < self.epsilon:\n",
    "            action = torch.rand(2)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.Q_network(obs)\n",
    "\n",
    "        action = torch.argmax(action)\n",
    "        if self.last_obs is not None and self.last_action is not None:\n",
    "            # store the transition\n",
    "            self.buffer.append((self.last_obs, self.last_action, obs, reward))\n",
    "\n",
    "        self.last_action = action\n",
    "        self.last_obs = obs\n",
    "        self.last_reward = reward\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        # return next action\n",
    "        return {\n",
    "            'confinement': bool(action),\n",
    "            'isolation': False,\n",
    "            'hospital': False,\n",
    "            'vaccinate': False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0AuQfnPZlXu8hlXToTPrHY",
     "type": "CODE"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_constant_exploration(policy: Agent, env: Env, iterations: int, options: dict = {}):\n",
    "    eval_trace, training_trace, loss_trace = [], [], []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Run an episode.\n",
    "        action_list, obs_list, cumulated_reward = run_episode(policy, env, weeks=30, seed=i)\n",
    "\n",
    "        # Log the cumuative reward to training trace\n",
    "        training_trace.append(cumulated_reward.item())\n",
    "\n",
    "        # run a training step :\n",
    "        loss_trace.append(policy.optimize_model())\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            # Evaluate the current policy with epsilon set to 0 (pure exploitation).\n",
    "            old_epsilon = policy.epsilon\n",
    "            policy.epsilon = 0\n",
    "            _, cumulated_reward_list, _ = evaluate_policy(policy, env, 20)\n",
    "            policy.epsilon = old_epsilon\n",
    "\n",
    "            # Compute the average cumulative reward over the 20 evaluation episodes and log it to eval trace.\n",
    "            #average_cumulated_reward = sum(cumulated_reward) / len(cumulated_reward_list)\n",
    "            average_cumulated_reward = sum(cumulated_reward_list) / len(cumulated_reward_list)\n",
    "            eval_trace.append(average_cumulated_reward.item())\n",
    "\n",
    "            \"\"\"\n",
    "            This shit is not asked\n",
    "            # saving the model if it performs better\n",
    "            if average_cumulated_reward > policy.best_eval_reward:\n",
    "                policy.save_model('models/best_model_1.pt')\n",
    "                policy.best_eval_reward = average_cumulated_reward\"\"\"\n",
    "\n",
    "            # print the average cumulative reward\n",
    "            print(f'Checkpoint {i // 50 + 1} : average cumulated reward = {average_cumulated_reward}')\n",
    "\n",
    "    return eval_trace, training_trace, loss_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "def hist_avg(*args, **kwargs) : pass\n",
    "\n",
    "def plot_training(training_traces, eval_trace):\n",
    "    plt.scatter(range(iterations), training_traces[0,:], label='training trace')\n",
    "    plt.scatter(range(iterations), training_traces[1,:], label='training trace')\n",
    "    plt.scatter(range(iterations), training_traces[2,:], label='training trace')\n",
    "    plt.plot(range(50, iterations + 1, 50), eval_trace, label='eval trace', color='red')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def training(train_fct, policy: Agent, iterations: int = 500, epochs: int = 3, options: dict = {}):\n",
    "    dyn = ModelDynamics('config/switzerland.yaml')\n",
    "    env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "            action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "            observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "            )\n",
    "\n",
    "    eval_traces, training_traces = [], []\n",
    "    best_eval_reward = -float('inf')\n",
    "    for i in range(epochs):\n",
    "        print(f'Training {i + 1 }')\n",
    "        obs, info = env.reset(seed=i)\n",
    "        options['seed'] = i\n",
    "        agent = policy(options)\n",
    "        eval_trace, training_trace, loss_trace = train_fct(agent, env, iterations=iterations, options=options)\n",
    "        eval_traces.append(eval_trace)\n",
    "        training_traces.append(training_trace)\n",
    "\n",
    "        if eval_trace[-1] > best_eval_reward:\n",
    "            best_eval_reward = eval_trace[-1]\n",
    "            if options['savepath'] is not None:\n",
    "                agent.save_model(options['savepath'])\n",
    "\n",
    "    eval_trace = np.mean(np.array(eval_traces), axis=0)\n",
    "    training_traces = np.array(training_traces)\n",
    "\n",
    "    plot_training(training_traces, eval_trace)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"update_episodes\": 5,  # target update rate : (fully) update the target network every 5 episodes\n",
    "    \"learning_rate\": 5e-3,  # learning rate : 5 · 10−3 (when the action-space is binary)\n",
    "    \"discount_factor\": .9,\n",
    "    \"buffer_size\": 2e4,\n",
    "    \"batch_size\": 2048,\n",
    "    \"epsilon\": .7,\n",
    "    \"savepath\": \"models/agentDQN_no_decreasing_exploration.pt\"\n",
    "}\n",
    "\n",
    "agentDQN1 = training(train_constant_exploration, AgentDQN, iterations=500, epochs=3, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_confined_days, cumulative_reward, number_of_total_deaths = evaluate_policy(agentDQN1, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(number_of_confined_days, cumulative_reward, number_of_total_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "obs, reward = env.reset()\n",
    "infos_list = []\n",
    "\n",
    "agentDQN1.reset()\n",
    "for i in range(30):\n",
    "    obs, reward, done, info = env.step(agentDQN1.act(obs, reward))\n",
    "    infos_list.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_results(*parse_infos(infos_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 3.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_decreasing_exploration(policy: Agent, env: Env, iterations: int, options: dict = {}):\n",
    "    \"\"\"\n",
    "    redefine the train function to update epsilon at each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # set the seed for reproducibility\n",
    "    eps_0 = options.get('eps_0', 0.7)\n",
    "    eps_min = options.get('eps_min', 0.2)\n",
    "\n",
    "    eval_trace, training_trace, loss_trace = [], [], []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # update epsilon\n",
    "        policy.epsilon = max(eps_0 * (iterations - i) / iterations, eps_min)\n",
    "\n",
    "        # Run an episode.\n",
    "        action_list, obs_list, cumulated_reward = run_episode(policy, env)\n",
    "\n",
    "        # Log the cumuative reward to training trace\n",
    "        training_trace.append(cumulated_reward.item())\n",
    "\n",
    "        # run a training step :\n",
    "        loss_trace.append(policy.optimize_model())\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            old_epsilon = policy.epsilon\n",
    "            policy.epsilon = 0\n",
    "            _, cumulated_reward_list, _ = evaluate_policy(policy, env, 20)\n",
    "            policy.epsilon = old_epsilon\n",
    "\n",
    "            # Compute the average cumulative reward over the 20 evaluation episodes and log it to eval trace.\n",
    "            average_cumulated_reward = (sum(cumulated_reward_list) / len(cumulated_reward_list)).item()\n",
    "            eval_trace.append(average_cumulated_reward)\n",
    "\n",
    "            # print the average cumulative reward\n",
    "            print(f'Checkpoint {i // 50 + 1} : average cumulated reward = {average_cumulated_reward}')\n",
    "\n",
    "            \"\"\"\n",
    "            This shit is not asked\n",
    "            # saving the model if it performs better\n",
    "            if average_cumulated_reward > policy.best_eval_reward:\n",
    "                policy.save_model('models/best_model_2.pth')\n",
    "                policy.best_eval_reward = average_cumulated_reward\"\"\"\n",
    "\n",
    "    return eval_trace, training_trace, loss_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"update_episodes\": 5,  # target update rate : (fully) update the target network every 5 episodes\n",
    "    \"learning_rate\": 5e-3,  # learning rate : 5 · 10−3 (when the action-space is binary)\n",
    "    \"discount_factor\": .9,\n",
    "    \"buffer_size\": 2e4,\n",
    "    \"batch_size\": 2048,\n",
    "    \"epsilon\": .7,\n",
    "    \"eps_min\": .2,\n",
    "    \"eps_0\": .7,\n",
    "    \"savepath\": \"models/agentDQN_decreasing_exploration.pth\"\n",
    "}\n",
    "\n",
    "agentDQN2 = training(train_decreasing_exploration, AgentDQN, iterations=500, epochs=3, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "agentDQN2.reset()\n",
    "number_of_confined_days, cumulative_reward, number_of_total_deaths = evaluate_policy(agentDQN2, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(number_of_confined_days, cumulative_reward, number_of_total_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "obs, reward = env.reset()\n",
    "infos_list = []\n",
    "\n",
    "agentDQN2.reset()\n",
    "for i in range(30):\n",
    "    obs, reward, done, info = env.step(agentDQN2.act(obs, reward))\n",
    "    infos_list.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_results(*parse_infos(infos_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 3.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for (agent, title) in [(agentDQN1, 'No decreasing exploration'), (agentDQN2, 'With decreasing exploration')]:\n",
    "    number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list = evaluate_policy(agent, env)\n",
    "    number_of_confined_days_array = np.array(number_of_confined_days_list, dtype=float).reshape((-1, 1))\n",
    "    cumulative_reward_array = np.array(cumulative_reward_list, dtype=float).reshape((-1, 1))\n",
    "    number_of_total_deaths_array = np.array(number_of_total_deaths_list, dtype=float).reshape((-1, 1))\n",
    "    fig, ax = plt.subplots(3, 1)\n",
    "    hist_avg(ax[0], number_of_total_deaths_array, 'deaths')\n",
    "    hist_avg(ax[1], number_of_confined_days_array, 'confinement days')\n",
    "    hist_avg(ax[2], cumulative_reward_array, 'cumulative rewards')\n",
    "    fig.tight_layout()\n",
    "    print(title)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 4.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 4.1.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class AgentDQNToggleAction(Agent):\n",
    "    \"\"\"\n",
    "    Implemented by following this tutorial : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \"\"\"\n",
    "    confinement = False\n",
    "    vaccinate = False\n",
    "    isolation = False\n",
    "    hospital = False\n",
    "\n",
    "    def __init__(self, seed=0, *args, **kwargs):\n",
    "        options = kwargs.get('options', {})\n",
    "\n",
    "        # set the seed for reproducibility\n",
    "        torch.manual_seed(options.get('seed', seed))\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "\n",
    "        self.update_episode = options.get('update_episode', 5)\n",
    "        self.learning_rate = options.get('learning_rate', 5e-3)\n",
    "        self.discount_factor = options.get('discount_factor', .9)\n",
    "        self.buffer_size = options.get('buffer_size', 2e4)\n",
    "        self.batch_size = options.get('batch_size', 2048)\n",
    "        self.epsilon = options.get('epsilon', .7)\n",
    "\n",
    "        self.episode = 0\n",
    "        self.buffer = []  # contains (states, actions, next_state, reward) tuples\n",
    "        self.last_obs, self.last_action, self.last_reward = None, None, None\n",
    "\n",
    "        in_dim, out_dim = 2 * 9 * 7, 5\n",
    "        self.Q_network = create_model(in_dim, out_dim, nn.ReLU())\n",
    "        self.Q_target_network = create_model(in_dim, out_dim, nn.ReLU())\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_network.parameters(), lr=self.learning_rate, amsgrad=True)\n",
    "\n",
    "        self.best_eval_reward = -float('inf')  # for model saving\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        self.Q_network.load_state_dict(torch.load(savepath))\n",
    "        # load the target network with the same weights\n",
    "        self.Q_target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0\n",
    "        transitions = torch.randperm(len(self.buffer))[:self.batch_size]\n",
    "        transitions = [self.buffer[i] for i in transitions]\n",
    "        state_batch, action_batch, next_state_batch, reward_batch = zip(*transitions)\n",
    "        state_batch, reward_batch = map(torch.stack, (state_batch, reward_batch))\n",
    "        action_batch = torch.tensor(action_batch).reshape(-1, 1)\n",
    "        state_batch = state_batch.to(device)\n",
    "        reward_batch = reward_batch.reshape((-1, 1)).to(device)  # reshape to (batch_size, 1)\n",
    "\n",
    "        next_state_batch = [obs for obs in next_state_batch if obs is not None]\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "        state_action_values = self.Q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            if len(next_state_batch) > 0:\n",
    "                non_final_mask = torch.tensor(tuple(map(lambda t: t[2] is not None, transitions)), device=device,\n",
    "                                              dtype=torch.bool)\n",
    "                next_state_batch = torch.stack(next_state_batch).to(device)\n",
    "                next_state_values[non_final_mask] = self.Q_target_network(next_state_batch).max(1)[0]\n",
    "\n",
    "        next_state_values = next_state_values.reshape((-1, 1))\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update the target network every update_episode steps\n",
    "        if self.episode % self.update_episode == 0:\n",
    "            self.Q_target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "        # do not do in place gradient clipping\n",
    "        return loss.item()\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        torch.save(self.Q_network.state_dict(), savepath)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Called at the beginning of each episode\n",
    "        \"\"\"\n",
    "        if self.last_reward is not None:\n",
    "            self.buffer.append((self.last_obs, self.last_action, None, self.last_reward))\n",
    "        self.last_reward = None\n",
    "        self.last_obs = None\n",
    "        self.last_action = None\n",
    "        self.episode += 1\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        # cast the observation\n",
    "        obs = observation2tensor(obs)\n",
    "        # get the next action\n",
    "        # epsilon-greedy policy\n",
    "        if torch.rand(1) < self.epsilon:\n",
    "            action = torch.rand(5)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.Q_network(obs)\n",
    "\n",
    "        action = torch.argmax(action)\n",
    "        if self.last_obs is not None and self.last_action is not None:\n",
    "            # store the transition\n",
    "            self.buffer.append((self.last_obs, self.last_action, obs, reward))\n",
    "\n",
    "        self.last_action = action\n",
    "        self.last_obs = obs\n",
    "        self.last_reward = reward\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        # return next action\n",
    "        action = [\n",
    "            'do nothing',\n",
    "            'toggle confinement',\n",
    "            'toggle isolation',\n",
    "            'toggle additional hospital beds',\n",
    "            'toggle vaccination'\n",
    "        ][action]\n",
    "        # return next action\n",
    "        toggle_confinement = action == 'toggle confinement'\n",
    "        toggle_isolation = action == 'toggle isolation'\n",
    "        toggle_hospital = action == 'toggle additional hospital beds'\n",
    "        toggle_vaccinate = action == 'toggle vaccination'\n",
    "        self.confinement = toggle_confinement ^ self.confinement\n",
    "        self.isolation = toggle_isolation ^ self.isolation\n",
    "        self.hospital = toggle_hospital ^ self.hospital\n",
    "        self.vaccinate = toggle_vaccinate ^ self.vaccinate\n",
    "        return {\n",
    "            'confinement': self.confinement,\n",
    "            'isolation': self.isolation,\n",
    "            'hospital': self.hospital,\n",
    "            'vaccinate': self.vaccinate\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# we have to train it 3 times and keep the best policy\n",
    "agents_toggle = []  # contains (agent, score, (eval_trace, training_trace, loss_trace)) tuples\n",
    "for i in range(3):\n",
    "    agentDQNToggleAction = AgentDQNToggleAction(seed=i)\n",
    "    eval_trace, training_trace, loss_trace = train_decreasing_exploration(agentDQNToggleAction, env, iterations=500,\n",
    "                                                                          eps_0=.7,\n",
    "                                                                          eps_min=.2)\n",
    "    agentDQNToggleAction.save_model(f'models/agentDQNToggleAction{i}.pt')\n",
    "    # run 3 episodes to evaluate the agent\n",
    "    for _ in range(3):\n",
    "        _, cumulative_reward_list, _ = evaluate_policy(agentDQNToggleAction, env)\n",
    "    agents_toggle.append(\n",
    "        (agentDQNToggleAction, np.mean(cumulative_reward_list), (eval_trace, training_trace, loss_trace)))\n",
    "agents_toggle.sort(key=lambda x: x[1], reverse=True)\n",
    "agentDQNToggleAction = agents_toggle[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# save the best agent\n",
    "agentDQNToggleAction.save_model('models/agentDQNToggleAction.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the training trace and evaluation trace, and the loss trace for each agent\n",
    "for i, (_, _, (eval_trace, training_trace, loss_trace)) in enumerate(agents_toggle):\n",
    "    plt.scatter(range(iterations), training_trace, label='training trace')\n",
    "    plt.plot(range(50, iterations + 1, 50), eval_trace, label='eval trace', color='red')\n",
    "    plt.title(f'Toggle action agent {i}')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(loss_trace)\n",
    "    plt.title(f'Toggle action agent {i}')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 4.1.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list = evaluate_policy(\n",
    "    agentDQNToggleAction, env)\n",
    "number_of_confined_days_array = np.array(number_of_confined_days_list, dtype=float).reshape((-1, 1))\n",
    "cumulative_reward_array = np.array(cumulative_reward_list, dtype=float).reshape((-1, 1))\n",
    "number_of_total_deaths_array = np.array(number_of_total_deaths_list, dtype=float).reshape((-1, 1))\n",
    "hist_avg(ax[0], number_of_total_deaths_array, 'deaths')\n",
    "hist_avg(ax[1], cumulative_reward_array, 'cumulative rewards')\n",
    "hist_avg(ax[2], number_of_confined_days_array, 'confined days')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 4.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 4.2.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class AgentDQNFactorized(Agent):\n",
    "    \"\"\"\n",
    "    Implemented by following this tutorial : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed=0, *args, **kwargs):\n",
    "        options = kwargs.get('options', {})\n",
    "\n",
    "        # set the seed for reproducibility\n",
    "        torch.manual_seed(options.get('seed', seed))\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "\n",
    "        self.update_episode = options.get('update_episode', 5)\n",
    "        self.learning_rate = options.get('learning_rate', 5e-3)\n",
    "        self.discount_factor = options.get('discount_factor', .9)\n",
    "        self.buffer_size = options.get('buffer_size', 2e4)\n",
    "        self.batch_size = options.get('batch_size', 2048)\n",
    "        self.epsilon = options.get('epsilon', .7)\n",
    "\n",
    "        self.episode = 0\n",
    "        self.buffer = []  # contains (states, actions, next_state, reward) tuples\n",
    "        self.last_obs, self.last_action, self.last_reward = None, None, None\n",
    "\n",
    "        in_dim, out_dim = 2 * 9 * 7, 4 * 2\n",
    "        self.Q_network = create_model(in_dim, out_dim, nn.ReLU())\n",
    "        self.Q_target_network = create_model(in_dim, out_dim, nn.ReLU())\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_network.parameters(), lr=self.learning_rate, amsgrad=True)\n",
    "\n",
    "        self.best_eval_reward = -float('inf')  # for model saving\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        self.Q_network.load_state_dict(torch.load(savepath))\n",
    "        # load the target network with the same weights\n",
    "        self.Q_target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0\n",
    "        transitions = torch.randperm(len(self.buffer))[:self.batch_size]\n",
    "        transitions = [self.buffer[i] for i in transitions]\n",
    "        state_batch, action_batch, next_state_batch, reward_batch = zip(*transitions)\n",
    "        state_batch, reward_batch = map(torch.stack, (state_batch, reward_batch))\n",
    "        action_batch = torch.stack(action_batch).reshape((-1, 4, 1))\n",
    "        state_batch = state_batch.to(device)\n",
    "        reward_batch = reward_batch.reshape((-1, 1)).to(device)  # reshape to (batch_size, 1)\n",
    "\n",
    "        next_state_batch = [obs for obs in next_state_batch if obs is not None]\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "        state_action_values = self.Q_network(state_batch).reshape((-1, 4, 2)).gather(2, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        next_state_values = torch.zeros((self.batch_size, 4), device=device)\n",
    "        with torch.no_grad():\n",
    "            if len(next_state_batch) > 0:\n",
    "                non_final_mask = torch.tensor(tuple(map(lambda t: t[2] is not None, transitions)), device=device,\n",
    "                                              dtype=torch.bool)\n",
    "                next_state_batch = torch.stack(next_state_batch).to(device)\n",
    "                next_state_values[non_final_mask] = self.Q_target_network(next_state_batch).reshape(-1, 4, 2).max(2)[0]\n",
    "\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        state_action_values = state_action_values.reshape((-1, 1))\n",
    "        expected_state_action_values = expected_state_action_values.reshape((-1, 1))\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update the target network every update_episode steps\n",
    "        if self.episode % self.update_episode == 0:\n",
    "            self.Q_target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "        # do not do in place gradient clipping\n",
    "        return loss.item()\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        torch.save(self.Q_network.state_dict(), savepath)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Called at the beginning of each episode\n",
    "        \"\"\"\n",
    "        if self.last_reward is not None :\n",
    "            self.buffer.append((self.last_obs, self.last_action, None, self.last_reward))\n",
    "\n",
    "        self.last_obs = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "        self.episode += 1\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        # cast the observation\n",
    "        obs = observation2tensor(obs)\n",
    "        # get the next action\n",
    "        # epsilon-greedy policy\n",
    "        if torch.rand(1) < self.epsilon:\n",
    "            action = torch.rand(8)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.Q_network(obs)\n",
    "\n",
    "        action = action.reshape((4, 2))\n",
    "        action = torch.tensor([np.argmax(col.numpy()) for col in action])\n",
    "        if self.last_obs is not None and self.last_action is not None:\n",
    "            # store the transition\n",
    "            self.buffer.append((self.last_obs, self.last_action, obs, reward))\n",
    "\n",
    "        self.last_action = action\n",
    "        self.last_obs = obs\n",
    "        self.last_reward = reward\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        # return next action\n",
    "        return {\n",
    "            'confinement': bool(action[0]),\n",
    "            'isolation': bool(action[1]),\n",
    "            'hospital': bool(action[2]),\n",
    "            'vaccinate': bool(action[3])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# we have to train it 3 times and keep the best policy\n",
    "agents_factorized = []  # contains (agent, score, (eval_trace, training_trace, loss_trace)) tuples\n",
    "for i in range(3):\n",
    "    agentDQNFactorized = AgentDQNFactorized(seed=i)\n",
    "    eval_trace, training_trace, loss_trace = train_decreasing_exploration(agentDQNFactorized, env, iterations=500,\n",
    "                                                                          eps_0=.7,\n",
    "                                                                          eps_min=.2)\n",
    "    agentDQNFactorized.save_model(f'models/agentDQNFactorized{i}.pt')\n",
    "    # run 3 episodes to evaluate the agent\n",
    "    for _ in range(3):\n",
    "        _, cumulative_reward_list, _ = evaluate_policy(agentDQNFactorized, env)\n",
    "    agents_factorized.append(\n",
    "        (agentDQNFactorized, np.mean(cumulative_reward_list), (eval_trace, training_trace, loss_trace)))\n",
    "agents_factorized.sort(key=lambda x: x[1], reverse=True)\n",
    "agentDQNFactorized = agents_factorized[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# save the best agent\n",
    "agentDQNFactorized.save_model('models/agentDQNFactorized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot the training trace and evaluation trace, and the loss trace for each agent\n",
    "for i, (_, _, (eval_trace, training_trace, loss_trace)) in enumerate(agents_factorized):\n",
    "    plt.scatter(range(iterations), training_trace, label='training trace')\n",
    "    plt.plot(range(50, iterations + 1, 50), eval_trace, label='eval trace', color='red')\n",
    "    plt.title(f'Factorized agent {i}')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(loss_trace)\n",
    "    plt.title(f'Factorized agent {i}')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation and training traces on a graph together with the traces from the toggle-action-space training (averaged across 3 training runs)\n",
    "toggle_eval_avg = np.mean([eval_trace for _, _, (eval_trace, _, _) in agents_toggle], axis=0)\n",
    "toggle_training_avg = np.mean([training_trace for _, _, (_, training_trace, _) in agents_toggle], axis=0)\n",
    "toggle_loss_avg = np.mean([loss_trace for _, _, (_, _, loss_trace) in agents_toggle], axis=0)\n",
    "\n",
    "factorized_eval_avg = np.mean([eval_trace for _, _, (eval_trace, _, _) in agents_factorized], axis=0)\n",
    "factorized_training_avg = np.mean([training_trace for _, _, (_, training_trace, _) in agents_factorized], axis=0)\n",
    "factorized_loss_avg = np.mean([loss_trace for _, _, (_, _, loss_trace) in agents_factorized], axis=0)\n",
    "\n",
    "plt.scatter(range(iterations), toggle_training_avg, label='toggle training trace')\n",
    "plt.scatter(range(iterations), factorized_training_avg, label='factorized training trace')\n",
    "plt.plot(range(50, iterations + 1, 50), toggle_eval_avg, label='toggle eval trace', color='red')\n",
    "plt.plot(range(50, iterations + 1, 50), factorized_eval_avg, label='factorized eval trace', color='green')\n",
    "plt.title('Comparison of training and eval traces')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('cumulative reward')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(toggle_loss_avg, label='toggle loss')\n",
    "plt.plot(factorized_loss_avg, label='factorized loss')\n",
    "plt.title('Comparison of loss traces')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Run a few episodes of the best policy π∗factor, to better understand the behavior of the learned policy.Plot one of those episodes and interpret the policy\n",
    "def plot_actions(actions: list) -> None:\n",
    "    ax_actions = plt.subplot2grid(shape=(18, 1), loc=(0, 0), colspan=1, rowspan=9)\n",
    "    ax_actions.imshow(np.array([list(v.values()) for v in actions]).astype(np.uint8).T, aspect='auto')\n",
    "    ax_actions.set_title('Actions')\n",
    "    ax_actions.set_yticks([0, 1, 2, 3])\n",
    "    ax_actions.set_yticklabels(list(actions[0].keys()))\n",
    "    ax_actions.set_xlabel('time (in weeks)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "actions, _, _ = run_episode(agentDQNFactorized, env)\n",
    "plot_actions(actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 4.2.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list = evaluate_policy(\n",
    "    agentDQNFactorized, env)\n",
    "number_of_confined_days_array = np.array(number_of_confined_days_list, dtype=float).reshape((-1, 1))\n",
    "cumulative_reward_array = np.array(cumulative_reward_list, dtype=float).reshape((-1, 1))\n",
    "number_of_total_deaths_array = np.array(number_of_total_deaths_list, dtype=float).reshape((-1, 1))\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "hist_avg(ax[0], number_of_total_deaths_array, 'deaths')\n",
    "hist_avg(ax[1], cumulative_reward_array, 'cumulative rewards')\n",
    "hist_avg(ax[2], number_of_confined_days_array, 'confined days')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [
    {
     "name": "gym",
     "source": "PIP",
     "version": "0.26.2"
    }
   ],
   "report_row_ids": [
    "U5fGYqNuBmGmfvzX3AUOZC",
    "ZISnysiDStsazjDQdpVXkE",
    "6rRndAIeGJ2DzMkTUyPBhf",
    "TQESyTp5G5m927c5nKdHfg",
    "Sdci8WTEw2t0KA46oyx5AR"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
