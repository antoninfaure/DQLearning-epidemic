{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "wdEXoCB8FXCtj8gN0SYOwR",
     "report_properties": {
      "rowId": "U5fGYqNuBmGmfvzX3AUOZC"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Mini-project 1: Deep Q-learning for Epidemic Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "yxT5wHtvLeP0DTwpKQmCuI",
     "report_properties": {
      "rowId": "ZISnysiDStsazjDQdpVXkE"
     },
     "type": "MD"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "kw5ZvUEfOAjmMgOeifSlMO",
     "report_properties": {
      "rowId": "6rRndAIeGJ2DzMkTUyPBhf"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env import Env\n",
    "from epidemic_env.dynamics import ModelDynamics\n",
    "from epidemic_env.agent import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "CMiRFc4yEUPN9bEij1sU5H",
     "report_properties": {
      "rowId": "TQESyTp5G5m927c5nKdHfg"
     },
     "type": "MD"
    }
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Vif12NGe2fONG7JMewjQ7j",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')\n",
    "env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "          action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "          observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "          )\n",
    "obs, info = env.reset(seed=0)\n",
    "action = {  # DO NOTHING\n",
    "    'confinement': False,\n",
    "    'isolation': False,\n",
    "    'hospital': False,\n",
    "    'vaccinate': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Pnjph7a8EAjnYVvI46JLJl",
     "report_properties": {
      "rowId": "Sdci8WTEw2t0KA46oyx5AR"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "infos_list = []\n",
    "for i in range(30):\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    infos_list.append(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "q7kZqh13zHLqoOCVf6SNlU",
     "type": "MD"
    }
   },
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "o58QmLRTOM8ZyuAliduPWr",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "totals = [info.total for info in infos_list]\n",
    "suceptibles = [total.suceptible for total in totals]\n",
    "exposeds = [total.exposed for total in totals]\n",
    "infected = [total.infected for total in totals]\n",
    "recovereds = [total.recovered for total in totals]\n",
    "deads = [total.dead for total in totals]\n",
    "\n",
    "plt.plot(suceptibles, label='suceptible')\n",
    "plt.plot(exposeds, label='exposed')\n",
    "plt.plot(infected, label='infected')\n",
    "plt.plot(recovereds, label='recovered')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "vg959ryEYzOzBjnwcdXL8j",
     "type": "MD"
    }
   },
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "b9xvR18cVxsyG8qQgKxxRv",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(infected, label='infected')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "2WPY1WNW7K9KJxHxnAWR5d",
     "type": "MD"
    }
   },
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "bscK4qE3sllo5OwX5Ku67B",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "cities = [info.city for info in infos_list]\n",
    "# take the first one to have the names\n",
    "city_names = list(cities[0])\n",
    "\n",
    "for name in city_names:\n",
    "    params = [city[name] for city in cities]\n",
    "    deads = [param.dead for param in params]\n",
    "    infected = [param.infected for param in params]\n",
    "    plt.plot(deads, label=f'dead')\n",
    "    plt.plot(infected, label=f'infected')\n",
    "    plt.xlabel('Time [week]')\n",
    "    plt.ylabel('# of people')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "qdwXW6noXBgeVjFLOgsccr",
     "type": "MD"
    }
   },
   "source": [
    "### Discuss the evolution of the variables over time.\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "uiEPSE2JW12oCASjj3dt5Q",
     "type": "MD"
    }
   },
   "source": [
    "## Question 2 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RussoPolicy(Agent):\n",
    "    def __init__(self):\n",
    "        self.total_infected = 0\n",
    "        self.confined = False\n",
    "        self.wi = 0\n",
    "        self.confinment_start = -1\n",
    "        self.confinment_end = -1\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_infected = 0\n",
    "        self.confined = False\n",
    "        self.wi = 0\n",
    "        self.confinment_start = -1\n",
    "        self.confinment_end = -1\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        self.total_infected += sum(obs.total.infected)\n",
    "        self.wi += 1\n",
    "        if self.confined:\n",
    "            if self.wi >= self.confinment_start + 4:\n",
    "                self.confinment_end = self.wi\n",
    "                self.confined = False\n",
    "            return {\n",
    "                'confinement': True,\n",
    "                'isolation': False,\n",
    "                'hospital': False,\n",
    "                'vaccinate': False,\n",
    "            }\n",
    "        if self.total_infected > 20000 and \\\n",
    "                self.confinment_end + 2 <= self.wi:\n",
    "            self.confined = True\n",
    "            self.confinment_start = self.wi\n",
    "        return {\n",
    "            'confinement': False,\n",
    "            'isolation': False,\n",
    "            'hospital': False,\n",
    "            'vaccinate': False,\n",
    "        }  #%% md\n",
    "\n",
    "# Mini-project 1: Deep Q-learning for Epidemic Mitigation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env import Env\n",
    "from epidemic_env.dynamics import ModelDynamics\n",
    "from epidemic_env.agent import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "\n",
    "% matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')\n",
    "env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "          action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "          observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "          )\n",
    "obs, info = env.reset(seed=0)\n",
    "action = {  # DO NOTHING\n",
    "    'confinement': False,\n",
    "    'isolation': False,\n",
    "    'hospital': False,\n",
    "    'vaccinate': False,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "infos_list = []\n",
    "for i in range(30):\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    infos_list.append(info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "totals = [info.total for info in infos_list]\n",
    "suceptibles = [total.suceptible for total in totals]\n",
    "exposeds = [total.exposed for total in totals]\n",
    "infected = [total.infected for total in totals]\n",
    "recovereds = [total.recovered for total in totals]\n",
    "deads = [total.dead for total in totals]\n",
    "\n",
    "plt.plot(suceptibles, label='suceptible')\n",
    "plt.plot(exposeds, label='exposed')\n",
    "plt.plot(infected, label='infected')\n",
    "plt.plot(recovereds, label='recovered')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(infected, label='infected')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cities = [info.city for info in infos_list]\n",
    "# take the first one to have the names\n",
    "city_names = list(cities[0])\n",
    "\n",
    "for name in city_names:\n",
    "    params = [city[name] for city in cities]\n",
    "    deads = [param.dead for param in params]\n",
    "    infected = [param.infected for param in params]\n",
    "    plt.plot(deads, label=f'dead')\n",
    "    plt.plot(infected, label=f'infected')\n",
    "    plt.xlabel('Time [week]')\n",
    "    plt.ylabel('# of people')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discuss the evolution of the variables over time.\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2 a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RussoPolicy(Agent):\n",
    "    def __init__(self):\n",
    "        self.total_infected = 0\n",
    "        self.confined = False\n",
    "        self.wi = 0\n",
    "        self.confinment_start = -1\n",
    "        self.confinment_end = -1\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        pass\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_infected = 0\n",
    "        self.confined = False\n",
    "        self.wi = 0\n",
    "        self.confinment_start = -1\n",
    "        self.confinment_end = -1\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        self.total_infected += sum(obs.total.infected)\n",
    "        self.wi += 1\n",
    "        if self.confined:\n",
    "            if self.wi >= self.confinment_start + 4:\n",
    "                self.confinment_end = self.wi\n",
    "                self.confined = False\n",
    "            return {\n",
    "                'confinement': True,\n",
    "                'isolation': False,\n",
    "                'hospital': False,\n",
    "                'vaccinate': False,\n",
    "            }\n",
    "        if self.total_infected > 20000 and \\\n",
    "                self.confinment_end + 2 <= self.wi:\n",
    "            self.confined = True\n",
    "            self.confinment_start = self.wi\n",
    "        return {\n",
    "            'confinement': False,\n",
    "            'isolation': False,\n",
    "            'hospital': False,\n",
    "            'vaccinate': False,\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')\n",
    "env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "          action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "          observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "          )\n",
    "obs, info = env.reset(seed=0)\n",
    "action = None\n",
    "infos_list = []\n",
    "policy = RussoPolicy()\n",
    "actions = []\n",
    "reward = [[0]]\n",
    "for i in range(30):\n",
    "    action = policy.act(obs, reward[0][0])\n",
    "    actions.append(action['confinement'])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    infos_list.append(info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "totals = [info.total for info in infos_list]\n",
    "suceptibles = [total.suceptible for total in totals]\n",
    "exposeds = [total.exposed for total in totals]\n",
    "infected = [total.infected for total in totals]\n",
    "recovereds = [total.recovered for total in totals]\n",
    "deads = [total.dead for total in totals]\n",
    "\n",
    "plt.plot(suceptibles, label='suceptible')\n",
    "plt.plot(exposeds, label='exposed')\n",
    "plt.plot(infected, label='infected')\n",
    "plt.plot(recovereds, label='recovered')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(infected, label='infected')\n",
    "plt.plot(deads, label='dead')\n",
    "plt.xlabel('Time [week]')\n",
    "plt.ylabel('# of people')\n",
    "plt.title('Total')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cities = [info.city for info in infos_list]\n",
    "# take the first one to have the names\n",
    "city_names = list(cities[0])\n",
    "\n",
    "for name in city_names:\n",
    "    params = [city[name] for city in cities]\n",
    "    deads = [param.dead for param in params]\n",
    "    infected = [param.infected for param in params]\n",
    "    plt.plot(deads, label=f'dead')\n",
    "    plt.plot(infected, label=f'infected')\n",
    "    plt.xlabel('Time [week]')\n",
    "    plt.ylabel('# of people')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow([actions, [not a for a in actions]], aspect='auto')\n",
    "# y ticks label['Confinment', 'Not confinment']\n",
    "plt.yticks([0, 1], ['Confinment', 'Not confinment'])\n",
    "plt.xlabel('Time [week]')\n",
    "plt.title('Actions')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2 b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_episode(policy: Agent, env:Env, weeks: int = 30, seed: int = 0) -> (list, list, ...):\n",
    "    # We pass a seed to the env to ensure reproductibility\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    policy.reset()\n",
    "    cumulative_reward = 0\n",
    "    action_list, obs_list, reward_list = [], [], []\n",
    "    reward = torch.tensor([[0]])\n",
    "    for i in range(weeks):\n",
    "        action = policy.act(obs, reward)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        cumulative_reward = reward + cumulative_reward\n",
    "        action_list.append(action)\n",
    "        obs_list.append(obs)\n",
    "        reward_list.append(reward)\n",
    "    return action_list, obs_list, cumulative_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_policy(policy: Agent, env:Env, iterations: int = 50) -> (list, list, list):\n",
    "    number_of_confined_days_list = []\n",
    "    cumulative_reward_list = []\n",
    "    number_of_total_deaths_list = []\n",
    "    for i in range(iterations):\n",
    "        # action_list, observation_list\n",
    "        action_list, obs_list, cumulative_reward = run_episode(policy, env, weeks=30, seed=i)\n",
    "        number_of_confined_days = np.sum([7 if action['confinement'] else 0 for action in action_list])\n",
    "        number_of_deaths = np.sum([np.sum(obs.total.dead) for obs in obs_list])\n",
    "        cumulative_reward_list.append(cumulative_reward)\n",
    "        number_of_confined_days_list.append(number_of_confined_days)\n",
    "        number_of_total_deaths_list.append(number_of_deaths)\n",
    "    cumulative_reward_list = [tensor[0, 0] for tensor in cumulative_reward_list]\n",
    "    return number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_of_confined_days_list, cumulative_reward_list, number_of_total_deaths_list = evaluate_policy(RussoPolicy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_of_confined_days_array = np.array(number_of_confined_days_list, dtype=float).reshape((-1, 1))\n",
    "cumulative_reward_array = np.array(cumulative_reward_list, dtype=float).reshape((-1, 1))\n",
    "number_of_total_deaths_array = np.array(number_of_total_deaths_list, dtype=float).reshape((-1, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hist_avg(ax, data, title):\n",
    "    ymax = 50\n",
    "    if title == 'deaths':\n",
    "        x_range = (1000, 200000)\n",
    "    elif title == 'cumulative rewards':\n",
    "        x_range = (-300, 300)\n",
    "    elif 'days' in title:\n",
    "        x_range = (0, 200)\n",
    "    else:\n",
    "        raise ValueError(f'{title} is not a valid title')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0, ymax)\n",
    "    ax.vlines([np.mean(data)], 0, ymax, color='red')\n",
    "    ax.hist(data, bins=60, range=x_range)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1)\n",
    "hist_avg(ax[0], number_of_total_deaths_array, 'deaths')\n",
    "hist_avg(ax[1], number_of_confined_days_array, 'confinment days')\n",
    "hist_avg(ax[2], cumulative_reward_array, 'cumulative rewards')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each deep-learning policy that you train in this project we recommend that you use the following\n",
    "# hyperparameters. If you wish so, you are free to test other values, but as searching through the hyperparameterspace can be quite a tedious experience we are giving you values that we know will allow the algorithm to converge\n",
    "# to a good policy\n",
    "def create_model(input_dim: int, output_dim: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, output_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "update_episodes = 5  # target update rate : (fully) update the target network every 5 episodes\n",
    "iterations = 500  # training length : train for 500 episodes\n",
    "learning_rate = 5e-3  # learning rate : 5 · 10−3 (when the action-space is binary)\n",
    "discount_factor = .9\n",
    "buffer_size = 2e4\n",
    "batch_size = 2048"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "zb77IxDo0RtXa4SAKFXit6",
     "type": "MD"
    }
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "TLgLHRGmxjSNps34SAUioB",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Note unless special measures are taken, the training of neural networks is non-deterministic in most deeplearning libraries. To ensure that your results are reproducible you will thus need to seed not only the environment but also your deep-learning library. Refer to the jupyter tutorial notebook for instructions.\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Mzq25aVMNiMHb1LhlWBkSC",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def observation2tensor(obs):\n",
    "    \"\"\"\n",
    "    Convert an Observation object to a torch tensor so it can be used to feed a nn\n",
    "    \"\"\"\n",
    "    # 9 cities, 2 for dead/infected, 7=|city.dead| days per week\n",
    "    # -> 2*7*9 sized vect\n",
    "    return torch.tensor([x for city in obs.city.values() for x in city.dead + city.infected]).float()\n",
    "\n",
    "\n",
    "observation2tensor(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ntcVgxLlpexN2KKUbg7sr8",
     "type": "MD"
    }
   },
   "source": [
    "### Question 3.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "PYeqWR4kxG0tUecp2h9PvA",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "yBp7CwtCMcWzXgndGgDtoH",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AgentDQN(Agent):\n",
    "    \"\"\"\n",
    "    Implemented by following this tutorial : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, *args, **kwargs):\n",
    "        self.buffer = []  # contains (states, actions, next_state, reward) tuples\n",
    "        self.last_obs = None\n",
    "        self.model = create_model(2 * 9 * 7, 2)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "    def load_model(self, savepath: str):\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "\n",
    "    def optimize_model(self) -> float:\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return 0\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, next_state_batch, reward_batch = zip(*transitions)\n",
    "        state_batch = torch.cat(state_batch)\n",
    "        action_batch = torch.cat(action_batch)\n",
    "        reward_batch = torch.cat(reward_batch)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.model(next_state_batch).max(1)[0]\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * discount_factor) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # do not do in place gradient clipping\n",
    "        return loss.item()\n",
    "\n",
    "    def save_model(self, savepath: str):\n",
    "        torch.save(self.model.state_dict(), savepath)\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_obs = None\n",
    "\n",
    "    def act(self, obs, reward):\n",
    "        # cast the observation\n",
    "        obs = observation2tensor(obs)\n",
    "        # get the next action\n",
    "        with torch.no_grad():\n",
    "            action = self.model(obs)\n",
    "        if self.last_obs is not None:\n",
    "            # store the transition\n",
    "            self.buffer.append((self.last_obs, action, obs, reward))\n",
    "        if len(self.buffer) > buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        # store the last observation\n",
    "        self.last_obs = obs\n",
    "        # return next action\n",
    "        return {\n",
    "            'confinement': bool(np.argmax(action.numpy())),\n",
    "            'isolation': False,\n",
    "            'hospital': False,\n",
    "            'vaccinate': False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0AuQfnPZlXu8hlXToTPrHY",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def train(policy: Agent):\n",
    "    env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "              action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "              observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "              )\n",
    "    eval_trace, training_trace = [], []\n",
    "    for i in range(iterations):\n",
    "        print(i)\n",
    "        # Run an episode.\n",
    "        action_list, obs_list, cumulated_reward = run_episode(policy, env)\n",
    "        # Log the cumuative reward to training trace\n",
    "        training_trace.append(cumulated_reward)\n",
    "        # run a training step :\n",
    "        policy.optimize_model()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            _, cumulated_reward_list, _ = evaluate_policy(policy, env, 20)\n",
    "            # Compute the average cumulative reward over the 20 evaluation episodes and log it to eval trace.\n",
    "            average_cumulated_reward = sum(cumulated_reward) / len(cumulated_reward_list)\n",
    "            eval_trace.append(average_cumulated_reward)\n",
    "            # save a checkpoint of the model\n",
    "            policy.save_model(f'checkpoint_{i}.pt')\n",
    "            print(f'Iteration {i} : average cumulated reward = {average_cumulated_reward}')\n",
    "    # plot the training trace and the eval trace. (We expect a plot of the reward\n",
    "    # in y and the training episode in x both for the training trace and the eval trace).\n",
    "    plt.plot(training_trace, range(iterations))\n",
    "    plt.plot(eval_trace, range(0, iterations, 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml')\n",
    "env = Env(dyn,  # We pass the dynamical model to the environment\n",
    "          action_space=None,  # Here one could pass an openai gym action space that can then be sampled\n",
    "          observation_space=None,  # Here one could pass an openai gym obs space that can then be sampled\n",
    "          )\n",
    "train(AgentDQN(env))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [
    {
     "name": "gym",
     "source": "PIP",
     "version": "0.26.2"
    }
   ],
   "report_row_ids": [
    "U5fGYqNuBmGmfvzX3AUOZC",
    "ZISnysiDStsazjDQdpVXkE",
    "6rRndAIeGJ2DzMkTUyPBhf",
    "TQESyTp5G5m927c5nKdHfg",
    "Sdci8WTEw2t0KA46oyx5AR"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
